{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a356e564",
   "metadata": {},
   "source": [
    "# Agentic AI Tutorial  \n",
    "## Chapter 1: Calling LLM Models  \n",
    "### Part 1: Introduction to Large Language Models (LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa04b0f",
   "metadata": {},
   "source": [
    "### 1. What is a Large Language Model?\n",
    "\n",
    "Large Language Models (**LLMs**) are the core building blocks of modern AI systems — including the **agentic AI** we are going to build in this tutorial.\n",
    "\n",
    "#### Simple Definition\n",
    "An **LLM** is an artificial intelligence model trained on enormous datasets (books, websites, code, conversations — basically vast portions of human-written text). Through this massive training, the model learns to:\n",
    "\n",
    "- Understand human language (grammar, context, meaning, facts, reasoning)\n",
    "- Predict the next **token** (word or subword) by calculating statistical probabilities\n",
    "- Generate new content that sounds very human-like (essays, code, summaries, chats, etc.)\n",
    "\n",
    "**Key Takeaway**: LLMs are essentially extremely good \"next-token predictors.\" This simple mechanism surprisingly enables complex reasoning, creativity, and task-solving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bbdea0",
   "metadata": {},
   "source": [
    "### 2. Why \"Large\"?\n",
    "\n",
    "The \"**Large**\" in LLM refers to two massive scales:\n",
    "\n",
    "1. **Parameters** — the model's \"memory knobs\" (tiny adjustable numbers learned during training). Modern models range from ~8 billion (good for local laptops) to trillions (frontier models like potential GPT-5 successors).\n",
    "2. **Training Data** — trillions of words/tokens, capturing a huge slice of human knowledge up to the training cutoff.\n",
    "\n",
    "This scale is what gives LLMs their impressive generalization and capabilities compared to smaller/earlier models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1970c80a",
   "metadata": {},
   "source": [
    "### 3. The Technology: Transformers\n",
    "\n",
    "Almost all powerful LLMs today are built using the **Transformer** architecture (introduced in the seminal 2017 paper **\"Attention Is All You Need\"** by Vaswani et al.).\n",
    "\n",
    "Key innovations in Transformers:\n",
    "- **Attention Mechanism** — lets the model focus on the most relevant parts of the input, even if words are far apart.\n",
    "- **Self-Attention** — processes all tokens in parallel (no sequential recurrence like in RNNs).\n",
    "- **Multi-Head Attention** — captures different types of relationships simultaneously.\n",
    "- **Positional Encoding** — adds information about token order since attention is permutation-invariant.\n",
    "- **Large Context Windows** — 2026 models often handle 128k–2M+ tokens (~100k–1.5M+ words) in one go.\n",
    "\n",
    "This architecture enabled massive parallelization, faster training, and better performance — kickstarting the LLM era.\n",
    "\n",
    "![Transformer Architecture Diagram from \"Attention Is All You Need\"](https://shreyansh26.github.io/assets/img/posts_images/attention/arch.PNG)\n",
    "*(Figure from explanations of the original paper — shows encoder/decoder stacks with multi-head attention, feed-forward layers, add & norm, and positional encodings.)*\n",
    "\n",
    "You can find the original paper here: [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2bf17d",
   "metadata": {},
   "source": [
    "#### Popular Models in 2026 (Quick Reference)\n",
    "\n",
    "| Provider   | Model Series              | Best For                        |\n",
    "|------------|---------------------------|---------------------------------|\n",
    "| OpenAI    | GPT-4o, o1, GPT-5 family  | Reasoning & Logic              |\n",
    "| Google    | Gemini 1.5 / 2.0 series   | Long Context & Multimodality   |\n",
    "| Anthropic | Claude 3.5 / 4            | Coding & Nuance                |\n",
    "| Meta      | Llama 3.1 / 4 series      | Open-source / Local hosting    |\n",
    "| Mistral   | Mistral Large 3           | Efficiency & Sovereignty       |\n",
    "\n",
    "*(Landscape evolves fast — check latest benchmarks like LMSYS Arena or Hugging Face Open LLM Leaderboard for updates!)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c01a96",
   "metadata": {},
   "source": [
    "### 4. From LLM to Agent\n",
    "\n",
    "Basic LLMs excel at **generating text** in response to prompts, but they are **reactive** — not autonomous agents.\n",
    "\n",
    "- **LLM** — A powerful \"brain\" that answers or generates based on input.\n",
    "- **Agent** — An LLM inside a loop that can:\n",
    "  - Reason step-by-step\n",
    "  - Use external **tools** (search, code execution, APIs…)\n",
    "  - Maintain **memory** across steps\n",
    "  - Make decisions and take actions toward a goal\n",
    "\n",
    "In this tutorial, we'll turn a simple LLM into a real agent using:\n",
    "\n",
    "- **LangChain** → Easy chaining, prompting, and memory\n",
    "- **LangGraph** → Controllable graph-based workflows & state machines\n",
    "- **Tool Calling** → Let the AI decide when/how to use functions\n",
    "- **Vector Databases** → Retrieval-Augmented Generation (RAG) for knowledge/memory\n",
    "\n",
    "But first: We need to **call** an LLM and get responses!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaac4b1",
   "metadata": {},
   "source": [
    "### 5. Summary & Checklist\n",
    "\n",
    "- [ ] **LLM** = Next-token predictor trained on massive data\n",
    "- [ ] Powered by **Transformers** + billions/trillions of parameters\n",
    "- [ ] Foundation for chat, code, translation, summarization…\n",
    "- [ ] **Starting point** for building autonomous **agents**\n",
    "\n",
    "**Pro Tip (2026 edition)**: Local models via Ollama (e.g., Llama-3.1-70B, Qwen2.5) are now very capable on consumer hardware — great for privacy & cost-free experimentation. Cloud APIs (Gemini, OpenAI) shine for frontier reasoning.\n",
    "\n",
    "Ready? see chapter1 code ipynb, we'll set up our environment and make our **first LLM call**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3b5bc2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
